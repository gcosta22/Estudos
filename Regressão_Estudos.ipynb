{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93180a76-7622-4cbe-8c7c-bed40a76f2fe",
   "metadata": {},
   "source": [
    "## ESTUDOS DE REGRESSÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583876a-6046-45b4-bdf8-6cbe0e607bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS, summarize, poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db6bbc-8935-4c86-8b3b-fbdbd19a11e6",
   "metadata": {},
   "source": [
    "# Função de Custo MSE\n",
    "\n",
    "A função de custo do Erro Quadrático Médio (MSE) é dada por:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "\n",
    "Onde:\n",
    "- $ h_\\theta(x^{(i)}) = \\theta^T x^{(i)} $ é a hipótese (predição do modelo).\n",
    "- $ y^{(i)} $ é o valor real.\n",
    "- $ m $ é o número de exemplos de treinamento.\n",
    "\n",
    "\n",
    "## Equação Normal\n",
    "\n",
    "A equação normal que minimiza $ J(\\theta) $ é:\n",
    "\n",
    "$$\n",
    "\\theta = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $ X $ é a matriz de características (com uma coluna de 1s para o termo de viés).\n",
    "- $ y $ é o vetor de valores reais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f3c9e-5f5b-4260-96e9-e1951a24872f",
   "metadata": {},
   "source": [
    "## Derivação da Equação Normal\n",
    "\n",
    "### 1. Função de Custo MSE\n",
    "\n",
    "A função de custo MSE é dada por:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T (X\\theta - y)\n",
    "$$\n",
    "\n",
    "### 2. Derivada da Função de Custo\n",
    "\n",
    "Para minimizar $ J(\\theta) $, derivamos em relação a $ \\theta $ e igualamos a zero:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{m} X^T (X\\theta - y) = 0\n",
    "$$\n",
    "\n",
    "### 3. Igualando a Derivada a Zero\n",
    "\n",
    "Rearranjando a equação, obtemos:\n",
    "\n",
    "$$\n",
    "X^T (X\\theta - y) = 0\n",
    "$$\n",
    "\n",
    "### 4. Resolvendo para $ \\theta $\n",
    "\n",
    "Isso nos leva a:\n",
    "\n",
    "$$\n",
    "X^T X\\theta = X^T y\n",
    "$$\n",
    "\n",
    "### 5. Equação Normal\n",
    "\n",
    "Finalmente, a solução para $ \\theta $ é:\n",
    "\n",
    "$$\n",
    "\\theta = (X^T X)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e5dbe1f-06a8-4253-ada2-f947589bb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumo = r\"\"\"\n",
    "### **RESUMO INTRODUCTION TO STATISTICAL LEARNING\n",
    "**3.1 Regressão Linear Simples** (p. 69)\n",
    "- **Definição**:\n",
    "  - Modela a relação entre uma variável resposta $ Y $ e uma única variável preditora $ X $.\n",
    "  - Equação do modelo: $ Y = \\beta_0 + \\beta_1 X + \\epsilon $.\n",
    "  - O objetivo é estimar os coeficientes $ \\beta_0 $ (intercepto) e $ \\beta_1 $ (inclinação) que minimizam o erro quadrático (RSS - Residual Sum of Squares).\n",
    "\n",
    "- **3.1.1 Estimação dos Coeficientes**:\n",
    "  - Os coeficientes são estimados usando o método dos **mínimos quadrados**.\n",
    "  - Fórmulas para $ \\beta_0 $ e $ \\beta_1 $:\n",
    "    $ \\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}, \\quad \\beta_0 = \\bar{y} - \\beta_1 \\bar{x}. $\n",
    "  - Essas fórmulas garantem que a linha de regressão seja a que melhor se ajusta aos dados, minimizando a soma dos quadrados dos resíduos.\n",
    "\n",
    "- **3.1.2 Avaliação do Modelo**:\n",
    "  - **RSE (Residual Standard Error)**: Mede o desvio padrão dos resíduos. Quanto menor o RSE, melhor o ajuste do modelo.\n",
    "    $ RSE = \\sqrt{\\frac{\\sum (y_i - \\bar{y})^2}{n - p - 1}} $\n",
    "  - **R² (Coeficiente de Determinação)**: Proporção da variância em $ Y $ explicada por $ X $.\n",
    "    $ R^2 = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum (y_i - \\bar{y})^2 $\n",
    "    Um valor de $ R^2 $ próximo de 1 indica que o modelo explica grande parte da variabilidade da resposta.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Regressão Linear Múltipla** (p. 81)\n",
    "- **Definição**:\n",
    "  - Modela a relação entre $ Y $ e várias variáveis preditoras $ X_1, X_2, \\dots, X_p $.\n",
    "  - Equação do modelo: $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon $.\n",
    "\n",
    "- **3.2.1 Estimação dos Coeficientes**:\n",
    "  - Os coeficientes são estimados minimizando o RSS, agora em um espaço multidimensional.\n",
    "  - A solução pode ser expressa em forma matricial:\n",
    "    $ \\hat{\\beta} = (X^T X)^{-1} X^T Y $\n",
    "  - Onde $ X $ é a matriz de preditores e $ Y $ é o vetor de respostas.\n",
    "\n",
    "- **3.2.2 Avaliação do Modelo**:\n",
    "  - **Teste F**: Verifica se pelo menos um dos preditores tem relação significativa com $ Y $.\n",
    "    $ F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} $\n",
    "    Um valor grande de $ F $ sugere que pelo menos um preditor é significativo.\n",
    "  - **Teste t**: Avalia a significância individual de cada coeficiente.\n",
    "    $ t = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)} $\n",
    "    Um valor absoluto grande de $ t $ indica que o preditor $ X_j $ é significativo.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3 Seleção de Variáveis** (p. 86)\n",
    "- **3.3.1 Métodos para Seleção**:\n",
    "  - **Seleção Forward**: Adiciona preditores um a um, começando com o modelo vazio. Em cada passo, o preditor que mais reduz o RSS é adicionado.\n",
    "  - **Seleção Backward**: Remove preditores um a um, começando com o modelo completo. Em cada passo, o preditor que menos contribui para o modelo é removido.\n",
    "  - **Seleção Mista**: Combina forward e backward, permitindo que preditores sejam adicionados ou removidos em cada passo.\n",
    "\n",
    "- **3.3.2 Critérios de Seleção**:\n",
    "  - **R² Ajustado**: Penaliza a adição de preditores irrelevantes.\n",
    "    $ R^2_{\\text{ajustado}} = 1 - \\frac{RSS/(n - p - 1)}{TSS/(n - 1)} $\n",
    "  - **Cp de Mallow**, **AIC**, **BIC**: Medem o trade-off entre ajuste e complexidade.\n",
    "    $ C_p = \\frac{RSS}{\\hat{\\sigma}^2} + 2p, \\quad AIC = n \\log(RSS) + 2p, \\quad BIC = n \\log(RSS) + p \\log(n) $\n",
    "\n",
    "---\n",
    "\n",
    "### **3.4 Interações e Termos Não Lineares** (p. 95)\n",
    "- **3.4.1 Interações**:\n",
    "  - Permitem que o efeito de um preditor dependa do valor de outro.\n",
    "  - Exemplo: $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon $.\n",
    "  - O termo de interação $ X_1 X_2 $ captura a sinergia entre os preditores.\n",
    "\n",
    "- **3.4.2 Termos Não Lineares**:\n",
    "  - Podem ser incluídos usando transformações (e.g., $ X^2 $, $ \\log(X) $).\n",
    "  - Exemplo: $ Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon $.\n",
    "  - Isso permite que o modelo capture relações não lineares entre $ X $ e $ Y $.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.5 Problemas Potenciais em Regressão Linear** (p. 100)\n",
    "- **3.5.1 Não Linearidade**:\n",
    "  - A relação entre $ Y $ e $ X $ pode não ser linear. Solução: usar transformações ou modelos não lineares.\n",
    "\n",
    "- **3.5.2 Correlação dos Erros**:\n",
    "  - Erros correlacionados podem invalidar inferências. Comum em dados temporais.\n",
    "\n",
    "- **3.5.3 Heterocedasticidade**:\n",
    "  - Variância dos erros não é constante. Solução: transformar $ Y $ ou usar mínimos quadrados ponderados.\n",
    "\n",
    "- **3.5.4 Outliers e Pontos de Alavancagem**:\n",
    "  - Observações extremas podem distorcer o modelo. Solução: identificar e remover outliers.\n",
    "\n",
    "- **3.5.5 Multicolinearidade**:\n",
    "  - Preditores altamente correlacionados podem inflar a variância das estimativas. Solução: remover preditores redundantes ou usar técnicas como PCA.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.6 Comparação com K-Nearest Neighbors (KNN)** (p. 111)\n",
    "- **3.6.1 KNN**:\n",
    "  - Método não paramétrico que prevê $ Y $ com base nos $ K $ vizinhos mais próximos.\n",
    "  - Vantagem: flexibilidade para capturar relações não lineares.\n",
    "  - Desvantagem: desempenho ruim em alta dimensionalidade (maldição da dimensionalidade).\n",
    "\n",
    "- **3.6.2 Comparação**:\n",
    "  - Regressão linear é melhor quando a relação verdadeira é linear ou próxima disso.\n",
    "  - KNN pode ser superior em relações não lineares, mas sofre com o aumento do número de preditores.\n",
    "\n",
    "---\n",
    "\n",
    "### **Equações Importantes**\n",
    "1. **Regressão Simples**:\n",
    "   $ Y = \\beta_0 + \\beta_1 X + \\epsilon $\n",
    "2. **Regressão Múltipla**:\n",
    "   $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon $\n",
    "3. **RSS (Residual Sum of Squares)**:\n",
    "   $ RSS = \\sum (y_i - \\hat{y}_i)^2 $\n",
    "4. **R²**:\n",
    "   $ R^2 = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum (y_i - \\bar{y})^2 $\n",
    "5. **Teste F**:\n",
    "   $ F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} $\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33e105bd-4865-444c-8580-8aef0f3dda0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### **RESUMO INTRODUCTION TO STATISTICAL LEARNING\n",
       "**3.1 Regressão Linear Simples** (p. 69)\n",
       "- **Definição**:\n",
       "  - Modela a relação entre uma variável resposta $ Y $ e uma única variável preditora $ X $.\n",
       "  - Equação do modelo: $ Y = \\beta_0 + \\beta_1 X + \\epsilon $.\n",
       "  - O objetivo é estimar os coeficientes $ \\beta_0 $ (intercepto) e $ \\beta_1 $ (inclinação) que minimizam o erro quadrático (RSS - Residual Sum of Squares).\n",
       "\n",
       "- **3.1.1 Estimação dos Coeficientes**:\n",
       "  - Os coeficientes são estimados usando o método dos **mínimos quadrados**.\n",
       "  - Fórmulas para $ \\beta_0 $ e $ \\beta_1 $:\n",
       "    $ \\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}, \\quad \\beta_0 = \\bar{y} - \\beta_1 \\bar{x}. $\n",
       "  - Essas fórmulas garantem que a linha de regressão seja a que melhor se ajusta aos dados, minimizando a soma dos quadrados dos resíduos.\n",
       "\n",
       "- **3.1.2 Avaliação do Modelo**:\n",
       "  - **RSE (Residual Standard Error)**: Mede o desvio padrão dos resíduos. Quanto menor o RSE, melhor o ajuste do modelo.\n",
       "    $ RSE = \\sqrt{\\frac{\\sum (y_i - bar{y})^2}{n - p - 1}} $\n",
       "  - **R² (Coeficiente de Determinação)**: Proporção da variância em $ Y $ explicada por $ X $.\n",
       "    $ R^2 = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum (y_i - \\bar{y})^2 $\n",
       "    Um valor de $ R^2 $ próximo de 1 indica que o modelo explica grande parte da variabilidade da resposta.\n",
       "\n",
       "---\n",
       "\n",
       "### **3.2 Regressão Linear Múltipla** (p. 81)\n",
       "- **Definição**:\n",
       "  - Modela a relação entre $ Y $ e várias variáveis preditoras $ X_1, X_2, \\dots, X_p $.\n",
       "  - Equação do modelo: $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon $.\n",
       "\n",
       "- **3.2.1 Estimação dos Coeficientes**:\n",
       "  - Os coeficientes são estimados minimizando o RSS, agora em um espaço multidimensional.\n",
       "  - A solução pode ser expressa em forma matricial:\n",
       "    $ \\hat{\\beta} = (X^T X)^{-1} X^T Y $\n",
       "  - Onde $ X $ é a matriz de preditores e $ Y $ é o vetor de respostas.\n",
       "\n",
       "- **3.2.2 Avaliação do Modelo**:\n",
       "  - **Teste F**: Verifica se pelo menos um dos preditores tem relação significativa com $ Y $.\n",
       "    $ F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} $\n",
       "    Um valor grande de $ F $ sugere que pelo menos um preditor é significativo.\n",
       "  - **Teste t**: Avalia a significância individual de cada coeficiente.\n",
       "    $ t = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)} $\n",
       "    Um valor absoluto grande de $ t $ indica que o preditor $ X_j $ é significativo.\n",
       "\n",
       "---\n",
       "\n",
       "### **3.3 Seleção de Variáveis** (p. 86)\n",
       "- **3.3.1 Métodos para Seleção**:\n",
       "  - **Seleção Forward**: Adiciona preditores um a um, começando com o modelo vazio. Em cada passo, o preditor que mais reduz o RSS é adicionado.\n",
       "  - **Seleção Backward**: Remove preditores um a um, começando com o modelo completo. Em cada passo, o preditor que menos contribui para o modelo é removido.\n",
       "  - **Seleção Mista**: Combina forward e backward, permitindo que preditores sejam adicionados ou removidos em cada passo.\n",
       "\n",
       "- **3.3.2 Critérios de Seleção**:\n",
       "  - **R² Ajustado**: Penaliza a adição de preditores irrelevantes.\n",
       "    $ R^2_{\\text{ajustado}} = 1 - \\frac{RSS/(n - p - 1)}{TSS/(n - 1)} $\n",
       "  - **Cp de Mallow**, **AIC**, **BIC**: Medem o trade-off entre ajuste e complexidade.\n",
       "    $ C_p = \\frac{RSS}{\\hat{\\sigma}^2} + 2p, \\quad AIC = n \\log(RSS) + 2p, \\quad BIC = n \\log(RSS) + p \\log(n) $\n",
       "\n",
       "---\n",
       "\n",
       "### **3.4 Interações e Termos Não Lineares** (p. 95)\n",
       "- **3.4.1 Interações**:\n",
       "  - Permitem que o efeito de um preditor dependa do valor de outro.\n",
       "  - Exemplo: $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon $.\n",
       "  - O termo de interação $ X_1 X_2 $ captura a sinergia entre os preditores.\n",
       "\n",
       "- **3.4.2 Termos Não Lineares**:\n",
       "  - Podem ser incluídos usando transformações (e.g., $ X^2 $, $ \\log(X) $).\n",
       "  - Exemplo: $ Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon $.\n",
       "  - Isso permite que o modelo capture relações não lineares entre $ X $ e $ Y $.\n",
       "\n",
       "---\n",
       "\n",
       "### **3.5 Problemas Potenciais em Regressão Linear** (p. 100)\n",
       "- **3.5.1 Não Linearidade**:\n",
       "  - A relação entre $ Y $ e $ X $ pode não ser linear. Solução: usar transformações ou modelos não lineares.\n",
       "\n",
       "- **3.5.2 Correlação dos Erros**:\n",
       "  - Erros correlacionados podem invalidar inferências. Comum em dados temporais.\n",
       "\n",
       "- **3.5.3 Heterocedasticidade**:\n",
       "  - Variância dos erros não é constante. Solução: transformar $ Y $ ou usar mínimos quadrados ponderados.\n",
       "\n",
       "- **3.5.4 Outliers e Pontos de Alavancagem**:\n",
       "  - Observações extremas podem distorcer o modelo. Solução: identificar e remover outliers.\n",
       "\n",
       "- **3.5.5 Multicolinearidade**:\n",
       "  - Preditores altamente correlacionados podem inflar a variância das estimativas. Solução: remover preditores redundantes ou usar técnicas como PCA.\n",
       "\n",
       "---\n",
       "\n",
       "### **3.6 Comparação com K-Nearest Neighbors (KNN)** (p. 111)\n",
       "- **3.6.1 KNN**:\n",
       "  - Método não paramétrico que prevê $ Y $ com base nos $ K $ vizinhos mais próximos.\n",
       "  - Vantagem: flexibilidade para capturar relações não lineares.\n",
       "  - Desvantagem: desempenho ruim em alta dimensionalidade (maldição da dimensionalidade).\n",
       "\n",
       "- **3.6.2 Comparação**:\n",
       "  - Regressão linear é melhor quando a relação verdadeira é linear ou próxima disso.\n",
       "  - KNN pode ser superior em relações não lineares, mas sofre com o aumento do número de preditores.\n",
       "\n",
       "---\n",
       "\n",
       "### **Equações Importantes**\n",
       "1. **Regressão Simples**:\n",
       "   $ Y = \\beta_0 + \\beta_1 X + \\epsilon $\n",
       "2. **Regressão Múltipla**:\n",
       "   $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon $\n",
       "3. **RSS (Residual Sum of Squares)**:\n",
       "   $ RSS = \\sum (y_i - \\hat{y}_i)^2 $\n",
       "4. **R²**:\n",
       "   $ R^2 = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum (y_i - \\bar{y})^2 $\n",
       "5. **Teste F**:\n",
       "   $ F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} $\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(resumo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3804a69d-b600-42eb-ac54-6541864a9066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X^T X:\n",
      " [[ 3  6]\n",
      " [ 6 14]]\n",
      "\n",
      "(X^T X)^(-1):\n",
      " [[ 2.33333333 -1.        ]\n",
      " [-1.          0.5       ]]\n",
      "\n",
      "X^T y:\n",
      " [ 6 14]\n",
      "\n",
      "Coeficientes theta (equação normal):\n",
      " [-1.77635684e-15  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])  # Matriz de características (com coluna de 1s para o viés)\n",
    "y = np.array([1, 2, 3])  # Vetor de valores reais\n",
    "\n",
    "# Passo 1: Calcular X^T X\n",
    "X_T_X = X.T @ X\n",
    "print(\"X^T X:\\n\", X_T_X)\n",
    "\n",
    "# Passo 2: Calcular a inversa de X^T X\n",
    "X_T_X_inv = np.linalg.inv(X_T_X)\n",
    "print(\"\\n(X^T X)^(-1):\\n\", X_T_X_inv)\n",
    "\n",
    "# Passo 3: Calcular X^T y\n",
    "X_T_y = X.T @ y\n",
    "print(\"\\nX^T y:\\n\", X_T_y)\n",
    "\n",
    "# Passo 4: Calcular theta\n",
    "theta = X_T_X_inv @ X_T_y\n",
    "print(\"\\nCoeficientes theta (equação normal):\\n\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ce212-ef90-41b7-8772-8c54aaf7ad29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e4ae4-d367-4c83-997d-7c2c5996b206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
