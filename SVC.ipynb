{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Support Vector Machines (SVMs)*\n",
    "\n",
    "Os *Support Vector Machines (SVMs)* são algoritmos de aprendizado de máquina supervisionado usados para classificação e regressão. Eles são particularmente eficazes em problemas de classificação linear e não linear, além de serem robustos em espaços de alta dimensionalidade. Resumo baseado no HOML, Aurélien Géron\n",
    "\n",
    "---\n",
    "\n",
    "## 1. *Linear SVM Classification*\n",
    "- *Objetivo*: Encontrar um hiperplano que melhor separe as classes em um espaço de características.\n",
    "- *Hiperplano*: Uma superfície linear (reta) que divide o espaço em duas regiões, uma para cada classe.\n",
    "- *Margem*: A distância entre o hiperplano e os pontos mais próximos de cada classe (vetores de suporte).\n",
    "- *Classificação Linear Perfeita*: Quando os dados são linearmente separáveis, o SVM encontra um hiperplano com a maior margem possível.\n",
    "- Importante normalizar as features!!!\n",
    "- *Equação do Hiperplano*:\n",
    "  $\n",
    "  f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
    "  $\n",
    "  Onde:\n",
    "  - $\\mathbf{w}$: Vetor de pesos.\n",
    "  - $b$: Termo de viés (bias).\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm.LinearSVC: Implementação eficiente para classificação linear. É possível especificar uma qunatidade de hiperparametros. como o C\n",
    "  - sklearn.svm.SVC com kernel='linear': Outra opção para classificação linear.\n",
    "  -SGDClassifier(loss-\"hinge\", alpha=1/(m*c))\n",
    "\n",
    "---\n",
    "\n",
    "## 2. *Soft Margin Classification*\n",
    "- *Problema*: Em dados não linearmente separáveis, um hiperplano rígido pode não existir.\n",
    "- *Solução*: Introduzir uma margem \"soft\" que permite alguns pontos estarem dentro da margem ou do lado errado do hiperplano.\n",
    "- *Parâmetro C*: Controla o trade-off entre maximizar a margem e minimizar os erros de classificação.\n",
    "  $\n",
    "  \\text{Minimizar: } \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
    "  $\n",
    "  Onde:\n",
    "  - $\\xi_i$: Violações da margem.\n",
    "  - $C$: Parâmetro de regularização.\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm.LinearSVC: Permite ajustar o parâmetro C para controlar a margem soft.\n",
    "  - sklearn.svm.SVC com kernel='linear': Também suporta margem soft.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. *Nonlinear SVM Classification*\n",
    "- *Problema*: Dados que não são linearmente separáveis no espaço original.\n",
    "- *Solução*: Mapear os dados para um espaço de maior dimensionalidade onde se tornam linearmente separáveis.\n",
    "- *Kernel Trick*: Usar funções de kernel para calcular similaridades no espaço transformado sem precisar calcular explicitamente as transformações.\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm.SVC: Suporta kernels não lineares, como RBF, polinomial e sigmoide.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. *Polynomial Kernel*\n",
    "- *Função de Kernel*:\n",
    "  $\n",
    "  K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^T \\mathbf{y} + c)^d\n",
    "  $\n",
    "  Onde:\n",
    "  - $d$: Grau do polinômio.\n",
    "  - $c$: Termo constante.\n",
    "- *Aplicação*: Útil para capturar relações polinomiais entre as características.\n",
    "- ALto grau polinomial pode deixar o modelo lento e kernel resolve isso\n",
    "- Grindsearch pode ajudar a encontrar valores melhores para hiperparametros \n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm.SVC com kernel='poly': Permite definir o grau do polinômio (degree) e o termo constante (coef0).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. *Similarity Features*\n",
    "- *Ideia*: Usar funções de similaridade para transformar os dados em um novo espaço onde a separação linear é possível.\n",
    "- Computa o quanto cada instância se assemelha a um ponto de referência específico escolhido no mapa \n",
    "- *Exemplo*: Medir a distância de cada ponto a pontos de referência (landmarks) para criar novas características.\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.metrics.pairwise: Contém funções para calcular similaridades, como cosine_similarity e rbf_kernel.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. *Gaussian RBF Kernel*\n",
    "- *Função de Kernel*:\n",
    "  $\n",
    "  K(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\gamma \\|\\mathbf{x} - \\mathbf{y}\\|^2\\right)\n",
    "  $\n",
    "  Onde:\n",
    "  - $\\gamma$: Controla a influência de cada ponto.\n",
    "- *Aplicação*: Útil para criar fronteiras de decisão não lineares suaves. Parecido com kernel polinomial\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm.SVC com kernel='rbf': Permite ajustar o parâmetro gamma.\n",
    "  -Com tantos para escolher, via de regra deve-se testar primeiro linear, priordialmente se conjunto de dados for grande e muitas caracteristicas. Se não for muito grande, pode testar gaussiano. Se tiver tempo livre (kkk) teste os outros usando validação cruzada e grind search\n",
    "\n",
    "---\n",
    "\n",
    "## 7. *Computational Complexity*\n",
    "- *Treinamento*: O tempo de treinamento do SVM depende do número de amostras e características.\n",
    "- *Kernel Linear*: Mais eficiente, com complexidade próxima de $ O(n \\cdot m) $.\n",
    "- *Kernel Não Linear*: Mais custoso, com complexidade entre $ O(n^2 \\cdot m) $ e $ O(n^3 \\cdot m) $.\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm.LinearSVC: Mais eficiente para problemas lineares.\n",
    "  - sklearn.svm.SVC: Mais flexível, mas pode ser mais lento para grandes conjuntos de dados.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. *SVM Regression*\n",
    "- *Objetivo*: Encontrar uma função que se ajuste aos dados enquanto mantém os resíduos dentro de uma margem $\\epsilon$.\n",
    "- *Margem $\\epsilon$*: Define uma \"zona de tolerância\" onde erros menores que \\(\\epsilon\\) são ignorados.\n",
    "- *Parâmetro C*: Controla o trade-off entre a complexidade do modelo e a tolerância a erros.\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm.SVR: Implementação de SVM para regressão, suporta kernels como RBF e polinomial.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. *Under the Hood*\n",
    "- *Funcionamento Interno*: O SVM resolve um problema de otimização convexa para encontrar o hiperplano ótimo.\n",
    "- *Vetores de Suporte*: Pontos mais próximos ao hiperplano que definem a margem e influenciam a decisão final.\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm: Contém todas as implementações de SVM, incluindo LinearSVC, SVC e SVR.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. *Decision Function and Predictions*\n",
    "- *Função de Decisão*:\n",
    "  $\n",
    "  f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
    "  $\n",
    "- *Previsões*: Se \\( f(\\mathbf{x}) > 0 \\), a amostra é classificada como positiva; caso contrário, como negativa.\n",
    "- *Pacotes Python*:\n",
    "  - model.decision_function(X): Retorna a pontuação da função de decisão para cada amostra.\n",
    "  - model.predict(X): Retorna as previsões finais.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. *Training Objective*\n",
    "- *Objetivo*: Minimizar:\n",
    "  $\n",
    "  \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
    "  $\n",
    "  Onde:\n",
    "  - \\(\\xi_i\\): Violações da margem.\n",
    "- *Restrições*:\n",
    "  $\n",
    "  y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\quad \\text{e} \\quad \\xi_i \\geq 0\n",
    "  $\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm: Implementa o problema de otimização internamente.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. *Quadratic Programming*\n",
    "- *Formato*: O problema de otimização do SVM é um problema de programação quadrática (QP).\n",
    "- *Solução*: Resolvido usando métodos como o algoritmo SMO (Sequential Minimal Optimization).\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm: Utiliza otimização interna para resolver o problema QP.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. *The Dual Problem*\n",
    "- *Formulação Dual*: Transforma o problema primal em uma forma que depende apenas dos produtos internos entre as amostras.\n",
    "- *Vantagem*: Permite o uso do kernel trick para lidar com dados não lineares.\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm: Implementa a formulação dual internamente.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. *Kernelized SVMs*\n",
    "- *Kernel Trick*: Substitui o produto interno $ \\mathbf{x}^T \\mathbf{y} $ por uma função de kernel $ K(\\mathbf{x}, \\mathbf{y}) $.\n",
    "- *Vantagem*: Permite trabalhar em espaços de alta dimensionalidade sem calcular explicitamente as transformações.\n",
    "- *Pacotes Python*:\n",
    "  - sklearn.svm.SVC: Suporta kernels como RBF, polinomial e sigmoide.\n",
    "\n",
    "---\n",
    "\n",
    "## Principais Pacotes Python para SVMs\n",
    "1. *Scikit-learn (sklearn)*:\n",
    "   - sklearn.svm.LinearSVC: Para classificação linear eficiente.\n",
    "   - sklearn.svm.SVC: Para classificação com suporte a kernels não lineares.\n",
    "   - sklearn.svm.SVR: Para regressão com SVM.\n",
    "   - sklearn.metrics.pairwise: Para calcular funções de kernel e similaridades.\n",
    "\n",
    "2. *Exemplo de Uso*:\n",
    "   ```python\n",
    "   from sklearn.svm import LinearSVC, SVC, SVR\n",
    "   from sklearn.datasets import make_classification\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.metrics import accuracy_score\n",
    "\n",
    "   # Dados de exemplo\n",
    "   X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "   # Linear SVM\n",
    "   model = LinearSVC(C=1.0)\n",
    "   model.fit(X_train, y_train)\n",
    "   y_pred = model.predict(X_test)\n",
    "   print(\"Acurácia (LinearSVC):\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "   # Kernel RBF SVM\n",
    "   model = SVC(kernel='rbf', gamma=0.1, C=1.0)\n",
    "   model.fit(X_train, y_train)\n",
    "   y_pred = model.predict(X_test)\n",
    "   print(\"Acurácia (SVC RBF):\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
