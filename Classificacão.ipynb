{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75facb29-4775-4296-a4e5-19722bdbab48",
   "metadata": {},
   "source": [
    "### RESUMO CLASSIFICAÇÃO E REGRESSÃO LOGÍSTICA\n",
    "\n",
    "**4.1 Visão Geral da Classificação**  \n",
    "* **Definição**:  \n",
    "   - A classificação é o processo de prever uma variável resposta qualitativa (categórica).  \n",
    "   - Diferente da regressão linear, que lida com respostas quantitativas, a classificação atribui observações a categorias ou classes.  \n",
    "   - Exemplos de aplicações: diagnóstico médico, detecção de fraudes, análise de DNA.  \n",
    "\n",
    "**4.2 Por Que Não Usar Regressão Linear?**  \n",
    "* **Problemas com Regressão Linear**:  \n",
    "   - Para respostas qualitativas com mais de duas classes, a codificação numérica (e.g., 1, 2, 3) implica uma ordenação artificial.  \n",
    "   - Para respostas binárias, a regressão linear pode prever probabilidades fora do intervalo [0, 1].  \n",
    "\n",
    "**4.3 Regressão Logística**  \n",
    "* **Definição**:  \n",
    "   - Modela a probabilidade de uma resposta binária usando a função logística.  \n",
    "   - Equação do modelo:  \n",
    "     $$\n",
    "     p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "     $$  \n",
    "   - A função logística garante que \\( p(X) \\) esteja sempre entre 0 e 1.  \n",
    "\n",
    "**4.3.1 O Modelo Logístico**  \n",
    "* **Interpretação dos Coeficientes**:  \n",
    "   - O log-odds (logaritmo da razão de chances) é linear em \\( X \\):  \n",
    "     $$\n",
    "     \\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X\n",
    "     $$  \n",
    "   - Um aumento de uma unidade em \\( X \\) multiplica as chances por $ e^{\\beta_1} $.  \n",
    "\n",
    "**4.3.2 Estimação dos Coeficientes**  \n",
    "* **Método de Máxima Verossimilhança**:  \n",
    "   - Os coeficientes $ \\beta_0 $ e $ \\beta_1 $ são estimados maximizando a função de verossimilhança:  \n",
    "     $$\n",
    "     l(\\beta_0, \\beta_1) = \\prod_{i: y_i = 1} p(x_i) \\prod_{i': y_{i'} = 0} (1 - p(x_{i'}))\n",
    "     $$  \n",
    "\n",
    "**4.3.3 Fazendo Previsões**  \n",
    "* **Exemplo**:  \n",
    "   - Para um indivíduo com \\( X = 1000 \\), a probabilidade de default é:  \n",
    "     $$\n",
    "     p(X) = \\frac{e^{-10.6513 + 0.0055 \\times 1000}}{1 + e^{-10.6513 + 0.0055 \\times 1000}} \\approx 0.00576\n",
    "     $$  \n",
    "\n",
    "**4.3.4 Regressão Logística Múltipla**  \n",
    "* **Quando usar?**:  \n",
    "   - Quando a variável resposta $Y$ é binária (ou seja, tem apenas duas categorias, como \"Sim/Não\", \"Sucesso/Fracasso\", \"Default/Não Default\").\n",
    "   - Exemplo: Prever o diagnóstico de um paciente com base em sintomas\n",
    "* **Extensão para Múltiplos Preditores**:  \n",
    "   - O modelo é estendido para:  \n",
    "     $$\n",
    "     \\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n",
    "     $$  \n",
    "   - A probabilidade é dada por:  \n",
    "     $$\n",
    "     p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p}}\n",
    "     $$  \n",
    "\n",
    "**4.3.5 Regressão Logística Multinomial**  \n",
    "* **Quando usar?**:  \n",
    "   - Quando a variável resposta $Y$ é multiclasse (ou seja, tem mais de duas categorias, como \"Stroke\", \"Drug Overdose\", \"Epileptic Seizure\").\n",
    "   - Exemplo: Prever se um cliente vai dar default (sim ou não) com base em variáveis como renda, saldo da conta e status de estudante.\n",
    "* **Para Respostas com \\( K > 2 \\) Classes**:  \n",
    "   - Uma classe é escolhida como baseline, e as probabilidades são modeladas como:  \n",
    "     $$\n",
    "     \\Pr(Y = k|X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1} x_1 + \\dots + \\beta_{kp} x_p}}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1} x_1 + \\dots + \\beta_{lp} x_p}}\n",
    "     $$  \n",
    "   - A codificação **softmax** trata todas as classes simetricamente:  \n",
    "     $$\n",
    "     \\Pr(Y = k|X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1} x_1 + \\dots + \\beta_{kp} x_p}}{\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1} x_1 + \\dots + \\beta_{lp} x_p}}\n",
    "     $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b59677-901b-4062-8207-312c22b304d9",
   "metadata": {},
   "source": [
    "### GENERATIVE MODELS FOR CLASSIFICATION  \n",
    "**4.4 Modelos Gerativos para Classificação**  \n",
    "* **Quando usar?**:  \n",
    "   - Modelos generativos são métodos de classificação que tentam modelar a distribuição dos dados de cada classe. Em vez de prever diretamente a classe de uma observação (como faz a regressão logística), eles:\n",
    "        -  Por exemplo, se a classe é \"Default\" ou \"Não Default\", o modelo aprende como são as distribuições de X (como saldo da conta, renda, etc.) para cada uma dessas classes.\n",
    "        -  Usam o teorema de Bayes para \"inverter\" essa modelagem e estimar a probabilidade de uma observação pertencer a cada classe.\n",
    "   - Quando se deseja modelar a distribuição dos preditores \\( X \\) dentro de cada classe \\( Y \\) e usar o teorema de Bayes para estimar $ \\Pr(Y = k | X = x) $.  \n",
    "   - Útil quando há separação substancial entre as classes ou quando a distribuição de \\( X \\) é aproximadamente normal.  \n",
    "   - Exemplo: Classificar pacientes em categorias de diagnóstico com base em sintomas.  \n",
    "\n",
    "* **Teorema de Bayes**:  \n",
    "   - A probabilidade posterior $ \\Pr(Y = k | X = x) $ é dada por:  \n",
    "     $$\n",
    "     \\Pr(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}\n",
    "     $$  \n",
    "     Onde:  \n",
    "     - $ \\pi_k $: Probabilidade a priori da classe \\( k \\).  \n",
    "     - $ f_k(x) $: Função de densidade de \\( X \\) na classe \\( k \\).  \n",
    "\n",
    "* **Vantagens dos Modelos Gerativos**:  \n",
    "   - São mais estáveis quando há separação substancial entre as classes.  \n",
    "   - Podem ser mais precisos que a regressão logística se a distribuição de \\( X \\) for aproximadamente normal e o tamanho da amostra for pequeno.  \n",
    "   - Podem ser estendidos naturalmente para problemas com mais de duas classes.  \n",
    "\n",
    "---\n",
    "\n",
    "**4.4.1 Análise Discriminante Linear (LDA) para \\( p = 1 \\)**  \n",
    "* **Suposições**:  \n",
    "   - \\( f_k(x) \\) é uma distribuição normal com média \\( \\mu_k \\) e variância \\( \\sigma^2 \\) (comum a todas as classes).  \n",
    "   - A função discriminante para a classe \\( k \\) é:  \n",
    "     $$\n",
    "     \\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n",
    "     $$  \n",
    "   - A observação \\( x \\) é classificada na classe com maior \\( \\delta_k(x) \\).  \n",
    "\n",
    "* **Estimação dos Parâmetros**:  \n",
    "   - \\( \\hat{\\mu}_k \\): Média dos valores de \\( X \\) na classe \\( k \\).  \n",
    "   - \\( \\hat{\\sigma}^2 \\): Variância ponderada entre as classes.  \n",
    "   - \\( \\hat{\\pi}_k \\): Proporção de observações na classe \\( k \\).  \n",
    "\n",
    "* **Exemplo**:  \n",
    "   - No conjunto de dados **Default**, o LDA classifica corretamente a maioria dos não-defaults, mas tem alta taxa de erro para os defaults.  \n",
    "\n",
    "---\n",
    "\n",
    "**4.4.2 Análise Discriminante Linear para \\( p > 1 \\)**  \n",
    "* **Suposições**:  \n",
    "   - \\( X \\) segue uma distribuição normal multivariada com vetor de média \\( \\mu_k \\) e matriz de covariância \\( \\Sigma \\) (comum a todas as classes).  \n",
    "   - A função discriminante para a classe \\( k \\) é:  \n",
    "     $$\n",
    "     \\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log(\\pi_k)\n",
    "     $$  \n",
    "\n",
    "* **Classificação**:  \n",
    "   - A observação \\( x \\) é classificada na classe com maior \\( \\delta_k(x) \\).  \n",
    "\n",
    "---\n",
    "\n",
    "**4.4.3 Análise Discriminante Quadrática (QDA)**  \n",
    "* **Suposições**:  \n",
    "   - Cada classe tem sua própria matriz de covariância \\( \\Sigma_k \\).  \n",
    "   - A função discriminante para a classe \\( k \\) é:  \n",
    "     $$\n",
    "     \\delta_k(x) = -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) - \\frac{1}{2} \\log|\\Sigma_k| + \\log(\\pi_k)\n",
    "     $$  \n",
    "\n",
    "* **Comparação com LDA**:  \n",
    "   - QDA é mais flexível, mas requer mais parâmetros.  \n",
    "   - LDA é preferível quando o número de observações é pequeno, enquanto QDA é melhor quando as matrizes de covariância das classes são diferentes.  \n",
    "\n",
    "---\n",
    "\n",
    "**4.4.4 Naive Bayes**  \n",
    "* **Suposições**:  \n",
    "   - Dentro de cada classe, os preditores são independentes.  \n",
    "   - A função de densidade conjunta é:  \n",
    "     $$\n",
    "     f_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\dots \\times f_{kp}(x_p)\n",
    "     $$  \n",
    "\n",
    "* **Estimação**:  \n",
    "   - Para preditores quantitativos, assume-se uma distribuição normal.  \n",
    "   - Para preditores qualitativos, usa-se a proporção de observações em cada categoria.  \n",
    "\n",
    "* **Vantagens**:  \n",
    "   - Simples e eficaz, especialmente quando \\( p \\) é grande ou \\( n \\) é pequeno.  \n",
    "   - Reduz a variância ao custo de introduzir viés.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c65146-e0b3-4352-8b4f-5dcb6d7b9206",
   "metadata": {},
   "source": [
    "### Tabela Resumo: Modelos de Classificação\n",
    "\n",
    "| **Modelo**                     | **Descrição**                                                                 | **Quando usar?**                                                                 | **Vantagens**                                                                 | **Desvantagens**                                                                 |\n",
    "|--------------------------------|-------------------------------------------------------------------------------|----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| **Regressão Logística Binária** | Modela \\( \\Pr(Y = 1 \\| X) \\) usando a função logística.                       | - Problemas de classificação binária. <br> - Quando a relação entre \\( X \\) e \\( Y \\) é aproximadamente linear. | - Simples e eficaz. <br> - Interpretação direta dos coeficientes.              | - Não funciona bem com classes separadas. <br> - Limitado a respostas binárias.   |\n",
    "| **Regressão Logística Múltipla** | Estende a regressão logística para múltiplos preditores.                      | - Quando há mais de um preditor e a resposta é binária. <br> - Problemas com múltiplas variáveis explicativas. | - Captura relações complexas entre preditores. <br> - Interpretação dos coeficientes. | - Pode sofrer com overfitting se \\( p \\) (número de preditores) for grande.       |\n",
    "| **Regressão Logística Multinomial** | Estende a regressão logística para respostas com mais de duas classes.         | - Quando a variável resposta tem \\( K > 2 \\) classes. <br> - Problemas multiclasse sem ordenação natural. | - Flexível para problemas multiclasse. <br> - Não assume ordenação entre classes. | - Requer mais dados para estimar parâmetros. <br> - Computacionalmente mais custoso. |\n",
    "| **LDA (Análise Discriminante Linear)** | Modelo generativo que assume distribuição normal com mesma matriz de covariância para todas as classes. | - Quando as classes têm distribuições normais com covariâncias homogêneas. <br> - Pequenos conjuntos de dados. | - Estável e eficiente. <br> - Funciona bem com poucos dados.                    | - Assume normalidade e homogeneidade das covariâncias. <br> - Menos flexível que QDA. |\n",
    "| **QDA (Análise Discriminante Quadrática)** | Modelo generativo que assume distribuição normal com matriz de covariância específica para cada classe. | - Quando as classes têm distribuições normais com covariâncias diferentes. <br> - Conjuntos de dados maiores. | - Mais flexível que LDA. <br> - Captura relações não lineares entre classes.     | - Requer mais dados para estimar parâmetros. <br> - Pode sofrer com overfitting.  |\n",
    "| **Naive Bayes**               | Modelo generativo que assume independência entre preditores dentro de cada classe. | - Quando os preditores são independentes ou o conjunto de dados é de alta dimensão. <br> - Problemas com muitas classes. | - Simples e rápido. <br> - Funciona bem com dados de alta dimensão.             | - A suposição de independência raramente é verdadeira. <br> - Pode perder precisão. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f962670-923d-4547-a602-1760f31c99ed",
   "metadata": {},
   "source": [
    "### RESUMO CLASSIFICAÇÃO HOML\n",
    "\n",
    "**3.1 Visão Geral da Classificação**  \n",
    "* **Definição**:  \n",
    "   - A classificação é o processo de prever uma variável resposta qualitativa (categórica).  \n",
    "   - Diferente da regressão, que prevê valores contínuos, a classificação atribui observações a categorias ou classes.  \n",
    "   - Exemplos de aplicações: reconhecimento de dígitos, diagnóstico médico, detecção de spam.\n",
    "     \n",
    "**3.4 Medidas de Desempenho**  \n",
    "* **Acurácia e Validação Cruzada**:  \n",
    "   - A acurácia pode ser enganosa, especialmente em conjuntos de dados desbalanceados.  \n",
    "   - A validação cruzada é uma técnica robusta para avaliar o desempenho do modelo: cross_val_score() \n",
    "* **Matriz de Confusão**:  \n",
    "   - A matriz de confusão fornece uma visão detalhada dos erros do classificador, mostrando quantas vezes uma classe foi confundida com outra.\n",
    "   - cross_val_predict()\n",
    "* **Precisão, Recall e F1-Score**:\n",
    "   - Depende do contexto em que eu estiver trabalhando.\n",
    "   - É poss[ivel ter uma precisao muito alta e atrapalhar o recall, depende do que eu quero olhar no modelo.    \n",
    "   - **Precisão**: Proporção de previsões positivas corretas.  \n",
    "     $$\n",
    "     \\text{Precisão} = \\frac{TP}{TP + FP}\n",
    "     $$  \n",
    "   - **Recall**: Proporção de instâncias positivas corretamente classificadas.  \n",
    "     $$\n",
    "     \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "     $$  \n",
    "   - **F1-Score**: Média harmônica entre precisão e recall.  \n",
    "     $$\n",
    "     F1 = 2 \\times \\frac{\\text{Precisão} \\times \\text{Recall}}{\\text{Precisão} + \\text{Recall}}\n",
    "     $$\n",
    "- É possivel ainda ploter uma curva Precisão vs Recall, para verificar o trade-off entre as duas \n",
    "     \n",
    "**3.5 Curva ROC e AUC**  \n",
    "* **Curva ROC**:  \n",
    "   - A curva ROC (Receiver Operating Characteristic) plota a taxa de verdadeiros positivos (TPR) contra a taxa de falsos positivos (FPR) para diferentes thresholds.\n",
    "   - A escolha por usar a curva precisao/recall (PR) ou ROC: Preferir PR sempre que classe positiva for rara ou qunado os falsos positivos forem mais importantes do que com os falsos negativos. Caso contrário, usar ROC\n",
    "* **AUC (Area Under the Curve)**:  \n",
    "   - A AUC mede a qualidade do classificador. Um valor próximo de 1 indica um bom desempenho.\n",
    "      \n",
    "**3.6 Classificação Multiclasse**  \n",
    "* **Estratégias Multiclasse**:  \n",
    "   - **One-vs-All (OvA)**: Treina um classificador binário para cada classe.  \n",
    "   - **One-vs-One (OvO)**: Treina um classificador binário para cada par de classes.  \n",
    "   - O Scikit-Learn automaticamente seleciona a estratégia adequada dependendo do algoritmo.\n",
    "  \n",
    "**3.7 Análise de Erros**  \n",
    "* **Matriz de Confusão Normalizada**:  \n",
    "   - A matriz de confusão normalizada ajuda a identificar quais classes são mais frequentemente confundidas.  \n",
    "* **Melhorias Possíveis**:  \n",
    "   - Coletar mais dados para classes problemáticas.  \n",
    "   - Engenharia de features para destacar padrões específicos.  \n",
    "   - Pré-processamento de imagens para reduzir ruídos e variações.\n",
    "  \n",
    "**3.8 Classificação Multilabel e Multioutput**  \n",
    "* **Classificação Multilabel**:  \n",
    "   - Um classificador multilabel pode atribuir múltiplas classes a uma única instância.  \n",
    "   - Exemplo: Reconhecimento de múltiplas pessoas em uma imagem.  \n",
    "* **Classificação Multioutput**:  \n",
    "   - Generalização da classificação multilabel, onde cada label pode ter múltiplos valores.  \n",
    "   - Exemplo: Remoção de ruído de imagens, onde cada pixel é uma label com valores de 0 a 255. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
